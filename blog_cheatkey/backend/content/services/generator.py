# d:\BlogCheatKey\blog_cheatkey_v2\blog_cheatkey\backend\content\services\generator.py
import re
import json
import logging
import time
import traceback
from urllib.parse import urlparse
from django.conf import settings
from konlpy.tag import Okt
from anthropic import Anthropic
from backend.research.models import ResearchSource, StatisticData
from backend.key_word.models import Keyword, Subtopic
from backend.content.models import BlogContent, MorphemeAnalysis
from backend.accounts.models import User
from .substitution_generator import SubstitutionGenerator
from .morpheme_analyzer import MorphemeAnalyzer 

logger = logging.getLogger(__name__)


class ContentGenerator:
    """
    Claude APIë¥¼ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ì½˜í…ì¸  ìƒì„± ì„œë¹„ìŠ¤
    - ìƒì„±ê³¼ ë™ì‹œì— ìµœì í™” ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì½˜í…ì¸  ìƒì„±
    """
    
    def __init__(self):
        self.anthropic_api_key = settings.ANTHROPIC_API_KEY
        self.model = "claude-3-5-sonnet-20240620" # Model updated
        self.client = Anthropic(api_key=self.anthropic_api_key)
        self.okt = Okt()
        self.max_retries = 3 # API í˜¸ì¶œ ì¬ì‹œë„ íšŸìˆ˜
        self.retry_delay = 5 # ì¬ì‹œë„ ê°„ê²© (ì´ˆ)
        self.substitution_generator = SubstitutionGenerator()
        self.morpheme_analyzer = MorphemeAnalyzer() # Instance of the new MorphemeAnalyzer
    
    def generate_content(self, keyword_id, user_id, target_audience=None, business_info=None, custom_morphemes=None, subtopics_list=None):
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ ë¸”ë¡œê·¸ ì½˜í…ì¸  ìƒì„± (ìµœì í™” ì¡°ê±´ ì¶©ì¡±)
        
        Args:
            keyword_id (int): í‚¤ì›Œë“œ ID
            user_id (int): ì‚¬ìš©ì ID
            target_audience (dict): íƒ€ê²Ÿ ë…ì ì •ë³´
            business_info (dict): ì‚¬ì—…ì ì •ë³´
            custom_morphemes (list): ì‚¬ìš©ì ì§€ì • í˜•íƒœì†Œ ëª©ë¡
            subtopics_list (list): ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬ëœ ì†Œì œëª© ëª©ë¡ (ê¸°ë³¸ê°’ None)
            
        Returns:
            int: ìƒì„±ëœ BlogContent ê°ì²´ì˜ ID, ì‹¤íŒ¨ ì‹œ None
        """
        for attempt in range(self.max_retries):
            try:
                keyword_obj = Keyword.objects.get(id=keyword_id)
                keyword_text = keyword_obj.keyword
                user = User.objects.get(id=user_id)
                
                current_subtopics = subtopics_list
                if current_subtopics is None:
                    current_subtopics = list(keyword_obj.subtopics.order_by('order').values_list('title', flat=True))
                
                news_sources = ResearchSource.objects.filter(keyword=keyword_obj, source_type='news')
                academic_sources = ResearchSource.objects.filter(keyword=keyword_obj, source_type='academic')
                general_sources = ResearchSource.objects.filter(keyword=keyword_obj, source_type='general')
                statistics = StatisticData.objects.filter(source__keyword=keyword_obj)
                
                existing_content = BlogContent.objects.filter(
                    keyword=keyword_obj, 
                    user=user, 
                    title__contains="(ìƒì„± ì¤‘...)"
                ).order_by('-created_at').first()
                
                data_for_prompt = {
                    "keyword": keyword_text,
                    "subtopics": current_subtopics,
                    "target_audience": target_audience or {
                        "primary": keyword_obj.main_intent or "ì¼ë°˜ ì‚¬ìš©ì",
                        "pain_points": keyword_obj.pain_points or ["ì •ë³´ ë¶€ì¡±"]
                    },
                    "business_info": business_info or {
                        "name": user.username,
                        "expertise": user.profile.expertise if hasattr(user, 'profile') and hasattr(user.profile, 'expertise') else "ê´€ë ¨ ë¶„ì•¼ ì „ë¬¸ê°€"
                    },
                    "custom_morphemes": custom_morphemes, 
                    "research_data": self._format_research_data(
                        news_sources, academic_sources, general_sources, statistics
                    )
                }
                
                logger.info(f"ì½˜í…ì¸  ìƒì„± API í˜¸ì¶œ ì‹œì‘ (ì‹œë„ {attempt+1}/{self.max_retries}): í‚¤ì›Œë“œ={keyword_text}, ì‚¬ìš©ì={user.username}")
                logger.info(f"ì½˜í…ì¸  ìƒì„±ì— ì‚¬ìš©ë˜ëŠ” ì†Œì œëª©: {current_subtopics}")

                prompt = self._create_optimized_content_prompt(data_for_prompt)
                
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=4096,
                    temperature=0.7,
                    messages=[{"role": "user", "content": prompt}]
                )
                
                logger.info("ì½˜í…ì¸  ìƒì„± API í˜¸ì¶œ ì™„ë£Œ")
                
                generated_content_text = response.content[0].text
                
                initial_analysis = self.morpheme_analyzer.analyze(generated_content_text, keyword_text, custom_morphemes)
                
                final_content_to_save = generated_content_text
                final_analysis_for_db = initial_analysis

                if not initial_analysis['is_fully_optimized']:
                    logger.info("1ì°¨ ìƒì„± ì½˜í…ì¸  ìµœì í™” í•„ìš”. ì¶”ê°€ ìµœì í™” ì‹œë„.")
                    logger.info(f"1ì°¨ ê²€ì¦ ê²°ê³¼: ê¸€ììˆ˜={initial_analysis['char_count']} (ìœ íš¨: {initial_analysis['is_valid_char_count']}), ëª©í‘œí˜•íƒœì†Œ ìœ íš¨={initial_analysis['is_valid_morphemes']}")
                    
                    optimization_prompt = self._create_verification_optimization_prompt(
                        generated_content_text, 
                        keyword_text, 
                        custom_morphemes,
                        initial_analysis
                    )
                    
                    optimization_response = self.client.messages.create(
                        model=self.model,
                        max_tokens=4096,
                        temperature=0.5,
                        messages=[{"role": "user", "content": optimization_prompt}]
                    )
                    
                    optimized_content_after_verify_prompt = optimization_response.content[0].text
                    analysis_after_verify_prompt = self.morpheme_analyzer.analyze(optimized_content_after_verify_prompt, keyword_text, custom_morphemes)
                    
                    logger.info(f"ì¶”ê°€ ìµœì í™” ì‹œë„ í›„ ê²°ê³¼: ê¸€ììˆ˜={analysis_after_verify_prompt['char_count']}, ëª©í‘œí˜•íƒœì†Œ ìœ íš¨={analysis_after_verify_prompt['is_valid_morphemes']}")

                    if self.morpheme_analyzer.is_better_optimization(analysis_after_verify_prompt, initial_analysis):
                        final_content_to_save = optimized_content_after_verify_prompt
                        final_analysis_for_db = analysis_after_verify_prompt
                        logger.info("ì¶”ê°€ ìµœì í™”ëœ ì½˜í…ì¸  ì‚¬ìš©: ë” ë‚˜ì€ ê²°ê³¼")
                    else:
                        logger.info("1ì°¨ ìƒì„± ì½˜í…ì¸  ì‚¬ìš©: ì¶”ê°€ ìµœì í™” í›„ ê°œì„ ë˜ì§€ ì•ŠìŒ")
                
                content_with_references = self._add_references(final_content_to_save, data_for_prompt['research_data'])
                mobile_formatted_content = self._format_for_mobile(content_with_references)
                references_list = self._extract_references(content_with_references)
                
                if existing_content:
                    existing_content.delete()
                
                blog_content = BlogContent.objects.create(
                    user=user,
                    keyword=keyword_obj,
                    title=f"{keyword_text} ì™„ë²½ ê°€ì´ë“œ", 
                    content=content_with_references,
                    mobile_formatted_content=mobile_formatted_content,
                    references=references_list,
                    char_count=final_analysis_for_db['char_count'],
                    is_optimized=final_analysis_for_db['is_fully_optimized'] 
                )
                
                logger.info("í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹œì‘")
                if 'morpheme_analysis' in final_analysis_for_db and 'counts' in final_analysis_for_db['morpheme_analysis']:
                    for morpheme, info in final_analysis_for_db['morpheme_analysis']['counts'].items():
                        MorphemeAnalysis.objects.create(
                            content=blog_content,
                            morpheme=morpheme,
                            count=info.get('count', 0),
                            is_valid=info.get('is_valid', False),
                            morpheme_type=info.get('type', 'unknown') # Save morpheme type
                        )
                
                logger.info(f"ì½˜í…ì¸  ìƒì„± ì™„ë£Œ: ID={blog_content.id}")
                return blog_content.id
                    
            except Exception as e:
                logger.error(f"ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt+1}/{self.max_retries}): {str(e)}")
                logger.error(traceback.format_exc())
                
                if 'overloaded_error' in str(e).lower() and attempt < self.max_retries - 1:
                    logger.warning(f"ì„œë²„ê°€ í˜¼ì¡í•©ë‹ˆë‹¤. {self.retry_delay}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    time.sleep(self.retry_delay)
                elif attempt == self.max_retries - 1:
                    logger.error("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬. ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨.")
                    if existing_content:
                        existing_content.title = f"{keyword_text} (ìƒì„± ì‹¤íŒ¨)"
                        existing_content.content = f"ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                        existing_content.save()
                    return None
                else:
                    time.sleep(self.retry_delay)
        
        logger.error(f"ëª¨ë“  ({self.max_retries}íšŒ) ì‹œë„ í›„ì—ë„ ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨: í‚¤ì›Œë“œ ID {keyword_id}")
        return None
                    
    def _format_research_data(self, news_sources, academic_sources, general_sources, statistics):
        research_data = {'news': [], 'academic': [], 'general': [], 'statistics': []}
        
        for source_type, sources in [('news', news_sources), ('academic', academic_sources), ('general', general_sources)]:
            for source in sources.order_by('-published_date')[:5]:
                research_data[source_type].append({
                    'title': source.title, 'url': source.url, 'snippet': source.snippet,
                    'date': source.published_date.isoformat() if source.published_date else '',
                    'source': source.author or urlparse(source.url).netloc
                })
        
        for stat in statistics.order_by('-source__published_date')[:5]:
            research_data['statistics'].append({
                'value': stat.value, 'context': stat.context, 'pattern_type': stat.pattern_type,
                'source_url': stat.source.url, 'source_title': stat.source.title,
                'source': stat.source.author or urlparse(stat.source.url).netloc,
                'date': stat.source.published_date.isoformat() if stat.source.published_date else ''
            })
        return research_data
    
    def _create_optimized_content_prompt(self, data):
        keyword = data["keyword"]
        custom_morphemes = data.get("custom_morphemes", [])

        # Use MorphemeAnalyzer to get categorized morphemes and their target ranges
        dummy_analysis = self.morpheme_analyzer.analyze("", keyword, custom_morphemes)
        base_morphemes = dummy_analysis['morpheme_analysis']['target_morphemes']['base']
        compound_morphemes = dummy_analysis['morpheme_analysis']['target_morphemes']['compound']

        target_min_base_morph = self.morpheme_analyzer.target_min_base_count
        target_max_base_morph = self.morpheme_analyzer.target_max_base_count
        target_min_compound_morph = self.morpheme_analyzer.target_min_compound_count
        target_max_compound_morph = self.morpheme_analyzer.target_max_compound_count

        target_min_chars = self.morpheme_analyzer.target_min_chars
        target_max_chars = self.morpheme_analyzer.target_max_chars

        keyword_instruction_parts = []
        
        # Instructions for base morphemes
        if base_morphemes:
            keyword_instruction_parts.append(f"- í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ ({', '.join(base_morphemes)}): ê°ê° {target_min_base_morph}-{target_max_base_morph}íšŒ ì´ë‚´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš© (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜'ì—ì„œ 'ì—”ì§„', 'ì˜¤ì¼', 'ì¢…ë¥˜' ê°ê° ì¹´ìš´íŠ¸)")

        # Instructions for compound morphemes/phrases
        if compound_morphemes:
            keyword_instruction_parts.append(f"- ë³µí•© í‚¤ì›Œë“œ ë° êµ¬ë¬¸ ({', '.join(compound_morphemes)}): ê°ê° {target_min_compound_morph}-{target_max_compound_morph}íšŒ ì´ë‚´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš© (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼', 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜')")
        
        keyword_instruction = "\n".join(keyword_instruction_parts)
        
        research_text = ""
        target_audience = data.get('target_audience', {})
        business_info = data.get('business_info', {})
        research_data_dict = data.get('research_data', {})

        if isinstance(research_data_dict, dict):
            news = research_data_dict.get('news', [])[:2]
            academic = research_data_dict.get('academic', [])[:2]
            general = research_data_dict.get('general', [])[:2]
            
            if news:
                research_text += "ğŸ“° ë‰´ìŠ¤ ìë£Œ:\n"
                for item in news: research_text += f"- {item.get('title', '')} ({item.get('source', '')}, {item.get('date','')}): {item.get('snippet', '')}\n"
            if academic:
                research_text += "\nğŸ“š í•™ìˆ  ìë£Œ:\n"
                for item in academic: research_text += f"- {item.get('title', '')} ({item.get('source', '')}, {item.get('date','')}): {item.get('snippet', '')}\n"
            if general:
                research_text += "\nğŸ” ì¼ë°˜ ìë£Œ:\n"
                for item in general: research_text += f"- {item.get('title', '')} ({item.get('source', '')}, {item.get('date','')}): {item.get('snippet', '')}\n"

        statistics_text = ""
        if isinstance(research_data_dict.get('statistics'), list) and research_data_dict.get('statistics'):
            statistics_text = "\nğŸ’¡ í™œìš© ê°€ëŠ¥í•œ í†µê³„ ìë£Œ (ìµœì†Œ 1ê°œ ì´ìƒ ë³¸ë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš©):\n"
            for stat in research_data_dict['statistics'][:3]:
                date_info = f" ({stat.get('date', '')[:4]}ë…„)" if stat.get('date') and len(stat.get('date')) >=4 else ""
                statistics_text += f"- {stat.get('context', '')}{date_info} (ì¶œì²˜: {stat.get('source_title', stat.get('source','ì•Œ ìˆ˜ ì—†ìŒ'))})\n"
        else:
            statistics_text = "\n(í™œìš© ê°€ëŠ¥í•œ íŠ¹ì • í†µê³„ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ê²½í–¥ì´ë‚˜ ì¤‘ìš”ì„±ì„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.)\n"


        optimization_requirements = f"""
        âš ï¸ ì¤‘ìš”: ë‹¤ìŒ ìµœì í™” ì¡°ê±´ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

        1. ê¸€ììˆ˜ ì¡°ê±´: ì •í™•íˆ {target_min_chars}-{target_max_chars}ì (ê³µë°± ì œì™¸, ì°¸ê³ ìë£Œ ì„¹ì…˜ ì œì™¸)
        - ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ê±°ë‚˜ í•„ìš”ì‹œ í™•ì¥í•˜ì—¬ ì´ ë²”ìœ„ì— ë§ì¶”ê¸°

        2. í‚¤ì›Œë“œ ë° ì£¼ìš” í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜ ì¡°ê±´:
        {keyword_instruction}
        - ì¤‘ìš”: Ctrl+Fë¡œ ê²€ìƒ‰í–ˆì„ ë•Œ ìœ„ì— ì–¸ê¸‰ëœ ëª¨ë“  í‚¤ì›Œë“œì™€ í˜•íƒœì†Œê°€ ê°ê° ì§€ì •ëœ ë²”ìœ„ ë‚´ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤!

        3. í‚¤ì›Œë“œ ìµœì í™” ë°©ë²•:
        - ì§€ì‹œì–´ í™œìš©: "{keyword}ëŠ”" â†’ "ì´ê²ƒì€" ë“±
        - ìì—°ìŠ¤ëŸ¬ìš´ ìƒëµ: ë¬¸ë§¥ìƒ ì´í•´ ê°€ëŠ¥í•œ ê²½ìš° ìƒëµ
        - ë™ì˜ì–´/ìœ ì‚¬ì–´ ëŒ€ì²´: ê³¼ë‹¤ ì‚¬ìš©ëœ ë‹¨ì–´ë¥¼ ì ì ˆí•œ ë™ì˜ì–´ë¡œ ëŒ€ì²´ (ë‹¨, ëª©í‘œ í˜•íƒœì†ŒëŠ” ìœ ì§€)

        âœ“ ìµœì¢… ê²€ì¦: ìƒì„± ì™„ë£Œ í›„, ìœ„ì— ì–¸ê¸‰ëœ ëª¨ë“  ëª©í‘œ í‚¤ì›Œë“œ/í˜•íƒœì†Œê°€ **ê°ê°** ì§€ì •ëœ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€, ê¸€ììˆ˜ê°€ ë§ëŠ”ì§€ **ë°˜ë“œì‹œ** ì¬í™•ì¸í•˜ì„¸ìš”. **ìµœëŒ€ íšŸìˆ˜ë¥¼ ë‹¨ 1íšŒë¼ë„ ì´ˆê³¼í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ì°¨ë¼ë¦¬ ìµœì†Œ íšŸìˆ˜ë³´ë‹¤ ì•½ê°„ ë¶€ì¡±í•œ ê²ƒì´ ë‚«ìŠµë‹ˆë‹¤.** ì´ ê·œì¹™ì€ ì ˆëŒ€ì ì…ë‹ˆë‹¤.
        """
        logger.info(f"í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬ë˜ëŠ” ì†Œì œëª©: {data.get('subtopics', [])}")
        subtopics_for_prompt = data.get('subtopics', [])
        subtopic_lines = ""
        if subtopics_for_prompt:
            for i, st_title in enumerate(subtopics_for_prompt):
                subtopic_lines += f"        ### {st_title}\n"
        else:
            subtopic_lines = "        (ì†Œì œëª© ì—†ì´ ììœ ë¡­ê²Œ ë³¸ë¡  êµ¬ì„±)\n"


        prompt = f"""
        ë‹¤ìŒ ì¡°ê±´ë“¤ì„ ì¤€ìˆ˜í•˜ì—¬ ì „ë¬¸ì„±ê³¼ ì¹œê·¼í•¨ì´ ì¡°í™”ëœ, ì½ê¸° ì‰½ê³  ì‹¤ìš©ì ì¸ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

        {optimization_requirements}

        í•„ìˆ˜ í™œìš© ìë£Œ (ë³¸ë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš©):
        {research_text if research_text else "(ì œê³µëœ íŠ¹ì • ë‰´ìŠ¤/í•™ìˆ /ì¼ë°˜ ìë£Œ ì—†ìŒ. ì¼ë°˜ì ì¸ ì •ë³´ í™œìš© ê°€ëŠ¥)"}
        
        í†µê³„ ìë£Œ (ë³¸ë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš©):
        {statistics_text}

        **ì¤‘ìš” ì°¸ê³ ìë£Œ ì¸ìš© ì§€ì¹¨:**
        1. ë³¸ë¬¸ì—ì„œ [1], [2]ì™€ ê°™ì€ ì¸ìš©ë²ˆí˜¸ í‘œì‹œëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
        2. ëŒ€ì‹  "X ë³´ê³ ì„œì— ë”°ë¥´ë©´" ë˜ëŠ” "Y ì—°êµ¬ ê²°ê³¼ì— ì˜í•˜ë©´" ë“± ì¶œì²˜ ì´ë¦„ì„ ì§ì ‘ ì–¸ê¸‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”. (ì˜ˆ: "í•œêµ­ì„ìœ ê³µì‚¬ ë³´ê³ ì„œì— ë”°ë¥´ë©´...")
        3. ì°¸ê³ ìë£Œì˜ ì¶œì²˜ëª…ê³¼ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
        4. ë§í¬ëŠ” ê¸€ í•˜ë‹¨ì˜ ì°¸ê³ ìë£Œ ì„¹ì…˜ì— ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ë¯€ë¡œ ë³¸ë¬¸ì— URLì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        5. ê° ì†Œì œëª© ì„¹ì…˜ì—ì„œ (ë˜ëŠ” ë³¸ë¡  ì „ë°˜ì— ê±¸ì³) ê´€ë ¨ ì°¸ê³ ìë£Œë¥¼ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì—¬ ì¸ìš©í•˜ì„¸ìš”.

        1. ê¸€ì˜ êµ¬ì¡°ì™€ í˜•ì‹
        - ì „ì²´ êµ¬ì¡°: ì„œë¡ (ì•½ 20%) - ë³¸ë¡ (ì•½ 60%) - ê²°ë¡ (ì•½ 20%)
        - ê° ì†Œì œëª©ì€ ### ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í‘œì‹œ
        - ì†Œì œëª© êµ¬ì„±:
{subtopic_lines}
        - ì „ì²´ ê¸¸ì´: {target_min_chars}-{target_max_chars}ì (ê³µë°± ì œì™¸)

        2. [í•„ìˆ˜] ì„œë¡  ì‘ì„± ê°€ì´ë“œ (ì „ë¬¸ì„± ë° í¥ë¯¸ ìœ ë°œ ê°•í™”)
        ì„œë¡ ì€ ë‹¤ìŒ 3ë‹¨ê³„ íë¦„ì„ ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.
        1. ë¬¸ì œ ì œê¸° ë° ê³µê°: ë…ìì˜ ê°€ì¥ í° ê³ ë¯¼({', '.join(target_audience.get('pain_points', []))})ì„ ì§ì ‘ ì–¸ê¸‰í•˜ë©° ì‹œì‘í•˜ì„¸ìš”. ê°€ëŠ¥í•˜ë‹¤ë©´ ì œê³µëœ í†µê³„/ì—°êµ¬ ìë£Œë¥¼ ì¸ìš©í•˜ì—¬ ë¬¸ì œì˜ ì‹¬ê°ì„±ì„ ë¶€ê°ì‹œí‚¤ì„¸ìš”. (ì˜ˆ: "í˜¹ì‹œ {keyword} ë•Œë¬¸ì— ê³¨ì¹˜ ì•„í”„ì‹ ê°€ìš”? ìµœê·¼ A ì—°êµ¬ì— ë”°ë¥´ë©´, ë§ì€ ë¶„ë“¤ì´ ë¹„ìŠ·í•œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤.")
        2. ì „ë¬¸ê°€ë¡œì„œì˜ ê¶Œìœ„ ì œì‹œ ë° í•´ê²°ì±… ì•”ì‹œ: {business_info.get('name', 'ì €í¬')}ì˜ ì „ë¬¸ì„±({business_info.get('expertise', 'ê´€ë ¨ ë¶„ì•¼ì˜ ê¹Šì€ ê²½í—˜')})ì„ ê°„ê²°í•˜ê²Œ ë“œëŸ¬ë‚´ì„¸ìš”. ë‹¨ìˆœíˆ 'í•´ê²°í•´ ì£¼ê² ë‹¤'ê°€ ì•„ë‹ˆë¼, 'ì™œ' ìš°ë¦¬ê°€ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. (ì˜ˆ: "ì§€ë‚œ Në…„ê°„ ì´ ë¶„ì•¼ë¥¼ ë‹¤ë¤„ì˜¨ ì „ë¬¸ê°€ë¡œì„œ, ì´ ë¬¸ì œì˜ í•µì‹¬ ì›ì¸ì´ ë¬´ì—‡ì¸ì§€ ëª…í™•íˆ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ì‚¬ì‹¤, ëŒ€ë¶€ë¶„ì˜ ë¬¸ì œëŠ” ê°„ë‹¨í•œ ì›ì¹™ ëª‡ ê°€ì§€ë§Œ ì•Œë©´ í•´ê²°ë©ë‹ˆë‹¤.")
        3. ê¸°ëŒ€ê° ì¦í­ ë° ë³¸ë¬¸ìœ¼ë¡œ ì—°ê²°: ì´ ê¸€ì„ ë‹¨ 5ë¶„ë§Œ íˆ¬ìí•´ ëê¹Œì§€ ì½ìœ¼ì‹œë©´, ë” ì´ìƒ {keyword} ë•Œë¬¸ì— ì‹œê°„ ë‚­ë¹„í•˜ì§€ ì•Šê³ , (ë…ìê°€ ì–»ì„ êµ¬ì²´ì  ì´ë“)ì„ ì–»ê²Œ ë˜ì‹¤ ê²ë‹ˆë‹¤. ë³¸ë¬¸ì—ì„œëŠ” ê·¸ í•µì‹¬ ë¹„ë²•ì„ ì†Œì œëª©ë³„ë¡œ ìì„¸íˆ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")

        3. ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼
        - ì „ë¬¸ê°€ì˜ ì§€ì‹ì„ ì‰½ê²Œ ì„¤ëª…í•˜ë“¯ì´ í¸ì•ˆí•œ í†¤ ìœ ì§€
        - ê° ë¬¸ë‹¨ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ìŒ ë¬¸ë‹¨ìœ¼ë¡œ ì—°ê²°
        - ìŠ¤í† ë¦¬í…”ë§ ìš”ì†Œ í™œìš© ê°€ëŠ¥
        - ì‹¤ì œ ì‚¬ë¡€ë‚˜ ë¹„ìœ ë¥¼ í†µí•´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…

        4. í•µì‹¬ í‚¤ì›Œë“œ í™œìš© (ìœ„ì˜ 'í‚¤ì›Œë“œ ë° ì£¼ìš” í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜ ì¡°ê±´' ì°¸ê³ )
        - ì£¼ í‚¤ì›Œë“œ: {keyword}
        - ê° ëª©í‘œ í‚¤ì›Œë“œ/í˜•íƒœì†Œë¥¼ ì§€ì •ëœ ë²”ìœ„ ë‚´ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©
            
        5. [í•„ìˆ˜] ì°¸ê³  ìë£Œ í™œìš©
        - ë³¸ë¡  ì „ë°˜ì— ê±¸ì³ ê´€ë ¨ í†µê³„/ì—°êµ¬ ìë£Œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš©
        - ì¸ìš©í•  ë•ŒëŠ” "~ì— ë”°ë¥´ë©´", "~ì˜ ì—°êµ¬ ê²°ê³¼", "~ì˜ ì¡°ì‚¬ì— ë”°ë¥´ë©´" ë“± ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©
        - ëª¨ë“  í†µê³„ì™€ ìˆ˜ì¹˜ëŠ” ì¶œì²˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ (ì˜ˆ: "2024ë…„ Z ê¸°ê´€ì˜ ì¡°ì‚¬ì— ë”°ë¥´ë©´...")
        - ê°€ëŠ¥í•œ ìµœì‹  ìë£Œë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©
        - í†µê³„ë‚˜ ìˆ˜ì¹˜ë¥¼ ì¸ìš©í•  ë•ŒëŠ” ê·¸ ì˜ë¯¸ë‚˜ ì‹œì‚¬ì ë„ í•¨ê»˜ ì„¤ëª…

        6. ë³¸ë¡  ì‘ì„± ê°€ì´ë“œ
        - ê° ì†Œì œëª©ë§ˆë‹¤ í•µì‹¬ ì£¼ì œ í•œ ì¤„ ìš”ì•½ìœ¼ë¡œ ì‹œì‘ ê°€ëŠ¥
        - ì´ë¡  â†’ ì‚¬ë¡€ â†’ ì‹¤ì²œ ë°©ë²• ìˆœìœ¼ë¡œ êµ¬ì„± ê°€ëŠ¥
        - ì°¸ê³  ìë£Œì˜ í†µê³„ë‚˜ ì—°êµ¬ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš©
        - ì „ë¬¸ì  ë‚´ìš©ë„ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
        - ê° ì„¹ì…˜ ëì—ì„œ ë‹¤ìŒ ì„¹ì…˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°

        7. ê²°ë¡  ì‘ì„± ê°€ì´ë“œ
        - ë³¸ë¡  ë‚´ìš© ìš”ì•½
        - ì‹¤ì²œ ê°€ëŠ¥í•œ ë‹¤ìŒ ë‹¨ê³„ ì œì‹œ
        - "{business_info.get('name', 'ì €í¬')}ê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆë‹¤ëŠ” ë©”ì‹œì§€" (ì„ íƒì )
        - ë…ìì™€ì˜ ìƒí˜¸ì‘ìš© ìœ ë„ (ì˜ˆ: ì§ˆë¬¸, ëŒ“ê¸€ ìš”ì²­)

        ìœ„ ì¡°ê±´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, íŠ¹íˆ íƒ€ê²Ÿ ë…ì({target_audience.get('primary', '')})ì˜ ì–´ë ¤ì›€ì„ í•´ê²°í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶”ì–´ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        return prompt
    
    def _create_verification_optimization_prompt(self, content, keyword, custom_morphemes, verification_result):
        morpheme_analysis_from_analyzer = verification_result.get('morpheme_analysis', {})
        
        base_morphemes = morpheme_analysis_from_analyzer['target_morphemes']['base']
        compound_morphemes = morpheme_analysis_from_analyzer['target_morphemes']['compound']
        current_counts = morpheme_analysis_from_analyzer['counts']

        target_min_base_morph = self.morpheme_analyzer.target_min_base_count
        target_max_base_morph = self.morpheme_analyzer.target_max_base_count
        target_min_compound_morph = self.morpheme_analyzer.target_min_compound_count
        target_max_compound_morph = self.morpheme_analyzer.target_max_compound_count

        target_min_chars = self.morpheme_analyzer.target_min_chars
        target_max_chars = self.morpheme_analyzer.target_max_chars
        
        morpheme_issues = []
        for morpheme in base_morphemes:
            info = current_counts.get(morpheme, {})
            count = info.get('count', 0)
            if not info.get('is_valid', True):
                morpheme_issues.append(f"- í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ '{morpheme}': í˜„ì¬ {count}íšŒ â†’ ëª©í‘œ {target_min_base_morph}-{target_max_base_morph}íšŒë¡œ ì¡°ì • í•„ìš”")
        
        for morpheme in compound_morphemes:
            info = current_counts.get(morpheme, {})
            count = info.get('count', 0)
            if not info.get('is_valid', True):
                morpheme_issues.append(f"- ë³µí•© í‚¤ì›Œë“œ/êµ¬ë¬¸ '{morpheme}': í˜„ì¬ {count}íšŒ â†’ ëª©í‘œ {target_min_compound_morph}-{target_max_compound_morph}íšŒë¡œ ì¡°ì • í•„ìš”")
        
        morpheme_issues_text = "\n".join(morpheme_issues) if morpheme_issues else "ëª¨ë“  ëª©í‘œ í˜•íƒœì†Œê°€ ì ì • ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤."
        
        char_count = verification_result['char_count']
        char_count_guidance = ""
        if char_count < target_min_chars:
            char_count_guidance = f"ê¸€ììˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í˜„ì¬ {char_count}ì. {target_min_chars-char_count}ì ì´ìƒ ëŠ˜ë ¤ {target_min_chars}-{target_max_chars}ìë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
        elif char_count > target_max_chars:
            char_count_guidance = f"ê¸€ììˆ˜ê°€ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ {char_count}ì. {char_count-target_max_chars}ì ì´ìƒ ì¤„ì—¬ {target_min_chars}-{target_max_chars}ìë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
        else:
            char_count_guidance = f"ê¸€ììˆ˜ëŠ” ì ì • ë²”ìœ„({target_min_chars}-{target_max_chars}ì)ì…ë‹ˆë‹¤. í˜•íƒœì†Œ ì¡°ì • ì‹œ ì´ ë²”ìœ„ë¥¼ ìœ ì§€í•´ì£¼ì„¸ìš”."
        
        optimization_strategies = self._generate_dynamic_optimization_strategies(keyword, current_counts, base_morphemes + compound_morphemes)
        
        return f"""
        ë‹¤ìŒ ë¸”ë¡œê·¸ ì½˜í…ì¸ ë¥¼ ìµœì í™”í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ì¡°ê±´ì„ ëª¨ë‘ ì¶©ì¡±í•˜ë„ë¡ ìˆ˜ì •í•´ì£¼ì„¸ìš”:
        
        ========== ìµœì í™” ëª©í‘œ ==========
        
        1. ê¸€ììˆ˜ ì¡°ê±´: {target_min_chars}-{target_max_chars}ì (ê³µë°± ì œì™¸)
           {char_count_guidance}
        
        2. ëª©í‘œ í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜ ì¡°ê±´:
           {morpheme_issues_text}

        3. í‚¤ì›Œë“œ ë° í˜•íƒœì†Œ ì¹´ìš´íŒ… ë°©ì‹:
           - í•µì‹¬ ê¸°ë³¸ í˜•íƒœì†Œ (ì˜ˆ: 'ì—”ì§„', 'ì˜¤ì¼', 'ì¢…ë¥˜'): ë¬¸ì¥ ë‚´ì—ì„œ ë¶€ë¶„ì ìœ¼ë¡œ í¬í•¨ë˜ì–´ë„ ì¹´ìš´íŠ¸ë©ë‹ˆë‹¤. (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜'ì—ì„œ 'ì—”ì§„' 1íšŒ, 'ì˜¤ì¼' 1íšŒ, 'ì¢…ë¥˜' 1íšŒ)
           - ë³µí•© í‚¤ì›Œë“œ ë° êµ¬ë¬¸ (ì˜ˆ: 'ì—”ì§„ì˜¤ì¼', 'ì—”ì§„ì˜¤ì¼ì¢…ë¥˜'): ì •í™•íˆ í•´ë‹¹ êµ¬ë¬¸ì´ ì¼ì¹˜í•´ì•¼ ì¹´ìš´íŠ¸ë©ë‹ˆë‹¤.

        ========== ìµœì í™” ì „ëµ ==========
        {optimization_strategies}
        
        ========== ì¤‘ìš” ì§€ì¹¨ ==========
        
        1. ì½˜í…ì¸ ì˜ í•µì‹¬ ë©”ì‹œì§€ì™€ ì „ë¬¸ì„±ì€ ìœ ì§€í•˜ì„¸ìš”.
        2. ëª¨ë“  ì†Œì œëª©ê³¼ ì£¼ìš” ì„¹ì…˜ì„ ìœ ì§€í•˜ì„¸ìš”.
        3. ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì²´ì™€ íë¦„ì„ ìœ ì§€í•˜ì„¸ìš”.
        4. ëª¨ë“  í†µê³„ ìë£Œ ì¸ìš©ê³¼ ì¶œì²˜ í‘œì‹œë¥¼ ìœ ì§€í•˜ì„¸ìš”.
        5. ì¡°ì • í›„ì—ëŠ” ë°˜ë“œì‹œ ìœ„ì— ì–¸ê¸‰ëœ ê° ëª©í‘œ í˜•íƒœì†Œê°€ ì§€ì •ëœ ë²”ìœ„ ë‚´ì—ì„œ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€, ê¸€ììˆ˜ê°€ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
        6. ê²°ê³¼ë¬¼ë§Œ ì œì‹œí•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
        
        ========== ì›ë³¸ ì½˜í…ì¸  ==========
        {content}
        """
    
    def _generate_dynamic_optimization_strategies(self, keyword, current_morpheme_counts, all_target_morphemes_list):
        excess_morphemes = []
        lacking_morphemes = []
        
        # Use MorphemeAnalyzer to get target ranges for each type
        ma = self.morpheme_analyzer # shorthand
        
        for morpheme in all_target_morphemes_list:
            info = current_morpheme_counts.get(morpheme, {})
            count = info.get('count', 0)
            morpheme_type = info.get('type')

            if morpheme_type == 'base':
                target_min = ma.target_min_base_count
                target_max = ma.target_max_base_count
            elif morpheme_type == 'compound':
                target_min = ma.target_min_compound_count
                target_max = ma.target_max_compound_count
            else: # Should not happen with proper categorization
                continue

            if count > target_max:
                excess_morphemes.append(morpheme)
            elif count < target_min:
                lacking_morphemes.append(morpheme)
        
        strategies = """
        1. ê³¼ë‹¤ ì‚¬ìš©ëœ ëª©í‘œ í˜•íƒœì†Œ ê°ì†Œ ë°©ë²•:
           - ë™ì˜ì–´/ìœ ì‚¬ì–´ ëŒ€ì²´: (ë‹¨, ëŒ€ì²´ì–´ëŠ” ëª©í‘œ í˜•íƒœì†Œê°€ ì•„ë‹ˆì–´ì•¼ í•˜ë©°, í•´ë‹¹ í˜•íƒœì†Œì˜ ì¹´ìš´íŒ… ë°©ì‹(ë¶€ë¶„/ì •í™•)ì„ ê³ ë ¤í•˜ì—¬ ëŒ€ì²´)
           - ì§€ì‹œì–´ ì‚¬ìš©: "ì´ê²ƒ", "ê·¸ê²ƒ", "í•´ë‹¹ ë‚´ìš©" ë“±ìœ¼ë¡œ ëŒ€ì²´
           - ìì—°ìŠ¤ëŸ¬ìš´ ìƒëµ: ë¬¸ë§¥ìƒ ì´í•´ ê°€ëŠ¥í•œ ê²½ìš° ìƒëµ
           - ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë¬¸ì¥ ì¬êµ¬ì„±: ê°™ì€ ì˜ë¯¸ë¥¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í‘œí˜„
           - í•´ë‹¹ í˜•íƒœì†Œê°€ í¬í•¨ëœ ë¬¸ì¥ ì „ì²´ë¥¼ ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ½ê²Œ ì‚­ì œí•˜ê±°ë‚˜, í˜•íƒœì†Œë§Œ ì œê±°í•˜ì—¬ ë¬¸ì¥ì„ ê°„ê²°í•˜ê²Œ ë§Œë“œì„¸ìš”.

        2. ë¶€ì¡±í•œ ëª©í‘œ í˜•íƒœì†Œ ì¦ê°€ ë°©ë²•:
           - êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì‚¬ë¡€ ì¶”ê°€: í•´ë‹¹ ëª©í‘œ í˜•íƒœì†Œê°€ í¬í•¨ëœ ì˜ˆì‹œ ì¶”ê°€
           - ì„¤ëª… í™•ì¥: í•µì‹¬ ê°œë…ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª… ì œê³µ (ëª©í‘œ í˜•íƒœì†Œ ì‚¬ìš©)
           - ì‹¤ìš©ì ì¸ íŒì´ë‚˜ ì¡°ì–¸ ì¶”ê°€: ëª©í‘œ í˜•íƒœì†Œê°€ í¬í•¨ëœ íŒ ì œì‹œ
           - ê¸°ì¡´ ë¬¸ì¥ ë¶„ë¦¬ ë˜ëŠ” í™•ì¥: í•œ ë¬¸ì¥ì„ ë‘ ê°œë¡œ ë‚˜ëˆ„ê±°ë‚˜ í™•ì¥í•˜ì—¬ ëª©í‘œ í˜•íƒœì†Œ ì‚¬ìš© ê¸°íšŒ ì¦ê°€
        """
        
        substitution_text = "\n3. ìœ ìš©í•œ ëŒ€ì²´ì–´ ì˜ˆì‹œ (ê³¼ë‹¤ í˜•íƒœì†Œ ê°ì†Œ ì‹œ):"
        added_subs = False
        
        # Suggest substitutions for excess morphemes
        for morpheme in excess_morphemes:
            morpheme_substitutions = self.substitution_generator.get_substitutions(keyword, morpheme)
            if morpheme_substitutions:
                substitution_text += f"\n   - '{morpheme}' ëŒ€ì²´ì–´: {', '.join(morpheme_substitutions[:3])}"
                added_subs = True
        
        return strategies + (substitution_text if added_subs else "")
        
    def _add_references(self, content, research_data):
        if "## ì°¸ê³ ìë£Œ" in content: return content
        
        references_to_add = []
        for source_type_key in ['news', 'academic', 'general']:
            for source_item in research_data.get(source_type_key, []):
                if self._find_citation_in_content(content, source_item):
                    if not any(ref['url'] == source_item.get('url') for ref in references_to_add if source_item.get('url')):
                        references_to_add.append({
                            'title': source_item.get('title', 'ì œëª© ì—†ìŒ'),
                            'url': source_item.get('url', '#'),
                            'source': source_item.get('source', '')
                        })
        
        for stat_item in research_data.get('statistics', []):
            source_url = stat_item.get('source_url', '')
            source_title = stat_item.get('source_title', '')
            if source_url and source_title and (source_title.lower() in content.lower() or (stat_item.get('source','').lower() in content.lower() and stat_item.get('source',''))):
                if not any(ref['url'] == source_url for ref in references_to_add):
                     references_to_add.append({
                        'title': source_title,
                        'url': source_url,
                        'source': stat_item.get('source', '')
                    })

        if not references_to_add: return content
        
        reference_section_text = "\n\n## ì°¸ê³ ìë£Œ\n"
        for i, ref_item in enumerate(references_to_add, 1):
            ref_source_text = f" - {ref_item['source']}" if ref_item['source'] else ""
            reference_section_text += f"{i}. [{ref_item['title']}]({ref_item['url']}){ref_source_text}\n"
        
        return content.strip() + reference_section_text

    def _extract_references(self, content_with_refs):
        extracted_refs = []
        if "## ì°¸ê³ ìë£Œ" in content_with_refs:
            refs_section_text = content_with_refs.split("## ì°¸ê³ ìë£Œ", 1)[1]
            link_pattern = re.compile(r'\[(.*?)\]\((.*?)\)(?: - (.*?))?\n')
            matches = link_pattern.findall(refs_section_text)
            
            for title, url, source in matches:
                extracted_refs.append({
                    'title': title.strip(), 'url': url.strip(),
                    'source': source.strip() if source else ''
                })
        return extracted_refs

    def _format_for_mobile(self, content):
        lines = content.split('\n')
        formatted_lines = []
        in_code_block = False

        for line in lines:
            stripped_line = line.strip()
            if stripped_line.startswith('```'):
                in_code_block = not in_code_block
                formatted_lines.append(line)
                continue
            
            if in_code_block or \
               stripped_line.startswith(('#', '##', '###')) or \
               not stripped_line or \
               stripped_line.startswith(('- ', '* ', '+ ')) or \
               re.match(r'^\d+\.\s', stripped_line) or \
               stripped_line.startswith('>'):
                formatted_lines.append(line)
                continue
            
            words = line.split()
            current_formatted_line = ""
            for word in words:
                temp_line_char_len = len((current_formatted_line + " " + word).replace(" ", ""))
                if temp_line_char_len > 23 and current_formatted_line:
                    formatted_lines.append(current_formatted_line)
                    current_formatted_line = word
                else:
                    current_formatted_line = (current_formatted_line + " " + word).strip()
            
            if current_formatted_line:
                formatted_lines.append(current_formatted_line)
        
        return '\n'.join(formatted_lines)
    
    def _find_citation_in_content(self, content_text, source_info_dict):
        content_lower = content_text.lower()
        
        source_name_candidates = [
            source_info_dict.get('source', '').lower(),
            source_info_dict.get('author', '').lower()
        ]
        source_name_candidates = [name for name in source_name_candidates if name and len(name) > 2]

        for name_candidate in source_name_candidates:
            if name_candidate in content_lower:
                return True
        
        title_lower = source_info_dict.get('title', '').lower()
        if title_lower:
            title_words = title_lower.split()
            for i in range(len(title_words) - 1):
                phrase = " ".join(title_words[i:i+2])
                if len(phrase) > 5 and phrase in content_lower: return True
            if len(title_words) >=3:
                phrase = " ".join(title_words[:3])
                if len(phrase) > 8 and phrase in content_lower: return True

        snippet_lower = source_info_dict.get('snippet', '').lower()
        if snippet_lower:
            numbers_in_snippet = re.findall(r'\d+(?:[.,]\d+)?%?', snippet_lower)
            key_phrases_in_snippet = re.findall(r'\b\w+\s\w+\s\w+\b', snippet_lower)

            citation_keywords = ["ë”°ë¥´ë©´", "ì—°êµ¬", "ì¡°ì‚¬", "ë³´ê³ ì„œ", "ë°œí‘œ", "í†µê³„", "ìë£Œ", "ì œì‹œ"]
            for num_in_snip in numbers_in_snippet:
                if num_in_snip in content_lower:
                    idx = content_lower.find(num_in_snip)
                    context_around_num = content_lower[max(0, idx-30):min(len(content_lower), idx+len(num_in_snip)+30)]
                    if any(cit_kw in context_around_num for cit_kw in citation_keywords):
                        return True
            
            for phrase_in_snip in key_phrases_in_snippet:
                if len(phrase_in_snip) > 8 and phrase_in_snip in content_lower:
                     idx = content_lower.find(phrase_in_snip)
                     context_around_phrase = content_lower[max(0, idx-30):min(len(content_lower), idx+len(phrase_in_snip)+30)]
                     if any(cit_kw in context_around_phrase for cit_kw in citation_keywords):
                        return True
        return False
